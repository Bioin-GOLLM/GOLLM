{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --user -U nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8GpUoqZolk4",
        "outputId": "f4d3154c-763d-46a9-b6f6-4cd8086b8f87"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IcxbScQpbHi",
        "outputId": "bd0f09c8-e1c2-4806-ebbf-cdd020e2b6fb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "import nltk\n",
        "\n",
        "# Load pre-trained BioBERT model for NER\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"alvaroalon2/biobert_genetic_ner\")\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"alvaroalon2/biobert_genetic_ner\")\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Example article text\n",
        "article_text = \"The TP53 gene encodes a tumor suppressor protein that regulates cell division and prevents cancer. BRCA1 and BRCA2 genes are associated with hereditary breast and ovarian cancer.\"\n",
        "\n",
        "# Tokenize the article text into words (preserve word-level segmentation)\n",
        "words = nltk.word_tokenize(article_text)\n",
        "\n",
        "# Tokenize and process the sentence\n",
        "inputs = tokenizer(words, is_split_into_words=True, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "input_ids = inputs[\"input_ids\"].to(device)\n",
        "attention_mask = inputs[\"attention_mask\"].to(device)\n",
        "\n",
        "# Run the model and get the predictions\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_ids, attention_mask=attention_mask)\n",
        "logits = outputs.logits\n",
        "\n",
        "# Get the predicted labels for each token\n",
        "predicted_labels = torch.argmax(logits, dim=2).cpu().numpy()\n",
        "\n",
        "# Convert the predicted labels to human-readable format\n",
        "label_map = model.config.id2label  # This maps the label indices to the actual label names\n",
        "\n",
        "# Extract entities from the text\n",
        "extracted_entities = []\n",
        "current_entity = None\n",
        "\n",
        "for word, label, token_id in zip(words, predicted_labels[0], input_ids[0].cpu().numpy()):\n",
        "    label_name = label_map[label]\n",
        "\n",
        "    # Convert token ID to token (wrap the token_id into a list for conversion)\n",
        "    token = tokenizer.convert_ids_to_tokens([token_id])[0]  # Extract the token from the list\n",
        "\n",
        "    if label_name == \"O\":  # Skip non-entity tokens\n",
        "        continue\n",
        "    elif label_name == \"B-GENETIC\":  # Start of a new entity\n",
        "        if current_entity:\n",
        "            extracted_entities.append(current_entity)\n",
        "        current_entity = token  # Start a new entity with the current token\n",
        "    elif label_name == \"I-GENETIC\":  # Continuation of an entity\n",
        "        if token.startswith(\"##\"):  # If it's a subword token, append to the entity\n",
        "            current_entity += token[2:]\n",
        "        else:\n",
        "            current_entity += \" \" + token\n",
        "\n",
        "# Add the last entity if it exists\n",
        "if current_entity:\n",
        "    extracted_entities.append(current_entity)\n",
        "\n",
        "# Print the extracted gene/protein entities\n",
        "for entity in extracted_entities:\n",
        "    print(f\"Entity: {entity}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmZ-Z5REgdeu",
        "outputId": "9fc3bfdb-e86a-44d4-b076-ac156ebc7933"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entity: TP53 gene\n",
            "Entity: BRCA1\n",
            "Entity: BRCA2\n"
          ]
        }
      ]
    }
  ]
}