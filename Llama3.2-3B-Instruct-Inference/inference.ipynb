{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.xpu.is_available():\n",
    "    device = torch.device('xpu')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS ONLY ONCE TO DOWNLOAD THE MODEL\n",
    "# IF YOU HAVE THE MODEL ALREADY, SKIP THIS CELL AND RUN load_model cell\n",
    "# https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct for llama3.2-3B-Instruct model\n",
    "def download_model(device):\n",
    "    '''\n",
    "    Downloads the model and tokenizer from the Hugging Face model hub and save it in current directory.\n",
    "    - need to specify the hugginface repo and model name\n",
    "    '''\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")  \n",
    "    tokenizer.save_pretrained(\".\")\n",
    "    model.save_pretrained(\".\")\n",
    "    return tokenizer, model\n",
    "\n",
    "tokenizer, model = download_model(device)\n",
    "print(f\"Model loaded on {device} in {time.time():.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS CELL IF YOU HAVE THE MODEL ALREADY\n",
    "# Change the path to where you saved the model (should be a folder with 9 files for Llama 3.2-3B-Instruct-model)\n",
    "model_path = \"C:/Users/aivan/Desktop/BIOIN 401/Prompt Engineering/Llama 3.2-3B-Instruct-model\"\n",
    "def load_model():\n",
    "    \"\"\"\n",
    "    Downloads the LLaMA model and tokenizer from the Hugging Face Hub.\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True).to(device)\n",
    "    \n",
    "    return tokenizer, model\n",
    "\n",
    "tokenizer, model = load_model()\n",
    "print(f\"Model loaded on {device} in {time.time():.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Run LLaMA Prompt\n",
    "def run_llama_prompt(model, tokenizer, prompt, max_new_tokens=256):\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    input_ids = inputs[\"input_ids\"].to(model.device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_k=50,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    completion = generated_text[len(prompt):].strip()\n",
    "    return completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Define Prompt and Execute Inference\n",
    "# you can use open() to read a file\n",
    "\n",
    "# scientific_text = (\n",
    "#     \"Phosphorylation of the C-terminal tail of the Drosophila protein TFIID is required for its activation \"\n",
    "#     \"as a transcription factor. Mutations in the taf7 gene that affect the phosphorylation of the C-terminal \"\n",
    "#     \"tail of TFIID impair its ability to bind to DNA and regulate transcription. The taf7 gene encodes a subunit \"\n",
    "#     \"of the TFIID complex that is essential for the assembly of the transcription factor on DNA. TFIID is a \"\n",
    "#     \"heterotrimeric complex composed of the TAF7 subunit, the TAF2 subunit, and the TBP subunit.\"\n",
    "# )\n",
    "scientific_text = open(\"C:/Users/aivan/Desktop/BIOIN 401/GOLLM/data/fetched_articles_subsets/subset_1/291061.txt\", \"r\").read()\n",
    "\n",
    "prompt = (\n",
    "    \"-Goal-\\n\"\n",
    "    \"You will read a short abstract or scientific text describing one or more genes or proteins.\\n\"\n",
    "    \"Determine if it explicitly describes or suggests 'transcription regulator activity' (GO:0140110) \"\n",
    "    \"or any closely related sub-activity (e.g., transcription repressor, transcription activator).\\n\\n\"\n",
    "    \"-Steps-\\n\"\n",
    "    \"1. Identify all gene/protein mentions in the text (e.g., official symbols, synonyms).\\n\"\n",
    "    \"2. For each identified gene/protein, check if it is associated with transcription regulator activity \"\n",
    "    \"or a direct sub-activity (like DNA-binding transcription factor activity).\\n\"\n",
    "    \"3. Output the results in JSON with the structure\\n\\n\"\n",
    "    \"{\\n\"\n",
    "    \"  \\\"genes\\\": [\\n\"\n",
    "    \"    {\\n\"\n",
    "    \"      \\\"gene_symbol\\\": \\\"string\\\",\\n\"\n",
    "    \"      \\\"transcription_regulator_activity\\\": \\\"true/false\\\",\\n\"\n",
    "    \"      \\\"evidence_snippet\\\": \\\"text that suggests the activity\\\"\\n\"\n",
    "    \"    }\\n\"\n",
    "    \"  ]\\n\"\n",
    "    \"}\\n\\n\"\n",
    "    \"-Example Format-\\n\"\n",
    "    \"{\\n\"\n",
    "    \"  \\\"genes\\\": [\\n\"\n",
    "    \"    {\\n\"\n",
    "    \"      \\\"gene_symbol\\\": \\\"TP53\\\",\\n\"\n",
    "    \"      \\\"transcription_regulator_activity\\\": \\\"true\\\",\\n\"\n",
    "    \"      \\\"evidence_snippet\\\": \\\"p53 is a transcription factor controlling cell cycle genes...\\\"\\n\"\n",
    "    \"    }\\n\"\n",
    "    \"  ]\\n\"\n",
    "    \"}\\n\\n\"\n",
    "    \"--Important--\\n\"\n",
    "    \"Return only the JSON, with no extra commentary, explanation, or bullet points.\\n\"\n",
    "    \"\\n\"\n",
    "    \"-Scientific Text-\\n\"\n",
    "    f\"{scientific_text}\\n\"\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "output = run_llama_prompt(model, tokenizer, prompt, max_new_tokens=256)\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"LLM Response\")\n",
    "print(output)\n",
    "print(f\"Time taken: {end_time - start_time:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BIOIN311C1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
