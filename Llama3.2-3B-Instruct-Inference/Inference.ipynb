{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AfSrrXBI8oi",
        "outputId": "9b9c344e-5f4d-4cdd-f148-3502a69c3405"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UWSHSonXlTeG"
      },
      "outputs": [],
      "source": [
        "# packages\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import time\n",
        "\n",
        "# from goatools import obo_parser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuXWNP8jcVkz"
      },
      "source": [
        "# 1. Pubtator API (TODO)\n",
        "- using the input folder, check if the file(s) have been pre-annotated by checking if the following APIs return something\n",
        "```\n",
        "https://www.ncbi.nlm.nih.gov/research/pubtator3-api/publications/export/biocxml?pmids={id}\n",
        "or\n",
        "https://www.ncbi.nlm.nih.gov/research/pubtator3-api/publications/pmc_export/biocxml?pmcids=PMC{id}\n",
        "```\n",
        "  - if annotated, retrieve it and put it in the output folder\n",
        "  - if not, use the example code they provide to send it to the server to be annotated"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duMZk-ubdK0K"
      },
      "source": [
        "# 2. Relevance (TODO)\n",
        "- This can be integrated with Gene Chunking. We can just check if the paper has genes (i.e check if there is an item in the dictionary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-Gak1R_RF8Y"
      },
      "source": [
        "*3. Gene Chunking*\n",
        "=============\n",
        "Given a **preannotated** article (check input folder) analyze the **XML** file via `parse_xml_file(xml_file_path)` which returns a dictionary of \"Gene Class\" from that file  \n",
        "\n",
        "The Gene Class has the following information:\n",
        "- self.gene_id: the gene id (from the xml file)\n",
        "- self.occurences: A list of occurences of the gene in the text. Each occurence is **3-sentences long**.\n",
        "    - The second sentence contains the explicit mention of the gene.\n",
        "    - The first and third sentences are the context of the gene mention.\n",
        "- self.symbol: the gene symbol (from NCBI. Initialized to None)\n",
        "- self.organism: the organism of the gene (from NCBI. Initialized to None)\n",
        "- self.full_name: the full name of the gene (from NCBI. Initialized to None. Set to name_from_article if not found in NCBI)\n",
        "- self.also_known_as: a list of other names of the gene (from NCBI. Initialized to None. Set to name_from_article if not found in NCBI)\n",
        "- self.name_from_article: the name of the gene as found in the article\n",
        "\n",
        "Steps:\n",
        "1. define the class Gene\n",
        "2. Parse the xml file\n",
        "3. update the Gene class with the information from NCBI\n",
        "Now you have a dictionary of Gene classes.~\n",
        "\n",
        "TIPS:\n",
        "- control + F \"sentence_buffer\" to find where you can adjust the buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9sSOV2PXqOJ",
        "outputId": "1db905b3-218c-4318-e0d7-0ea3b623136e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: biopython in /usr/local/lib/python3.11/dist-packages (1.85)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from biopython) (1.26.4)\n",
            "Gene(2670)\n",
            "  Symbol          : GFAP\n",
            "  Organism        : Homo sapiens\n",
            "  Full Name       : glial fibrillary acidic protein\n",
            "  Also Known As   : ALXDRD\n",
            "  In-text Name    : glial fibrillary acidic protein\n",
            "  Occurrences     : ['We found that a lectin, Datura stramonium agglutinin, induced irreversible differentiation in C6 glioma cells. The differentiated cells had long processes, a low rate of proliferation and a high content of glial fibrillary acidic protein. When the medium was replaced with Datura stramonium agglutinin-free medium after 1 h, cell proliferation continued to be inhibited.', 'Proliferation of four human glial tumour cells was also inhibited by Datura stramonium agglutinin. Further, these differentiated human glial tumour cells had long processes and a high content of glial fibrillary acidic protein similar to differentiated C6 glioma cells. Taken together, these observations suggest that Datura stramonium agglutinin may be useful as a new therapy for treating glioma without side effects.', 'Recently, we found that DSA induced differentiation of astrocytes. Addition of DSA caused a morphological change from a polygonal shape to a stellate shape with many long processes, an increase of glial fibrillary acidic protein (GFAP) and suppression of proliferation. Therefore, it was of interest to determine whether DSA can inhibit cell-specific proliferation and induce differentiation of glioma cells.', 'Increased expression of GFAP after addition of DSA', 'The malignancy of gliomas is inversely correlated with the content of astrocyte-specific intermediate filament protein (GFAP). Furthermore, mature astrocytes are characterised by the expression of a large amount of GFAP.', 'The malignancy of gliomas is inversely correlated with the content of astrocyte-specific intermediate filament protein (GFAP). Furthermore, mature astrocytes are characterised by the expression of a large amount of GFAP. GFAP is expressed exclusively in astrocytes and the expression level increases during differentiation.', 'Furthermore, mature astrocytes are characterised by the expression of a large amount of GFAP. GFAP is expressed exclusively in astrocytes and the expression level increases during differentiation. C6 cells were found to produce only a small amount of GFAP due to their malignant properties.', 'GFAP is expressed exclusively in astrocytes and the expression level increases during differentiation. C6 cells were found to produce only a small amount of GFAP due to their malignant properties. The immunoblot in Figure 3 shows clearly that the expression of GFAP was enhanced after the addition of DSA.', 'C6 cells were found to produce only a small amount of GFAP due to their malignant properties. The immunoblot in Figure 3 shows clearly that the expression of GFAP was enhanced after the addition of DSA. A densitometric evaluation of the blot indicated that the amount of GFAP after the addition of DSA increased over hundred fold relative to the amount in the control cells.', 'The immunoblot in Figure 3 shows clearly that the expression of GFAP was enhanced after the addition of DSA. A densitometric evaluation of the blot indicated that the amount of GFAP after the addition of DSA increased over hundred fold relative to the amount in the control cells. The corresponding values of human glial tumour cell lines were more than 12.2 times.', 'The corresponding values of human glial tumour cell lines were more than 12.2 times. Differentiated astrocytes are characterised by a stellate cell morphology, increased GFAP expression, and inhibited proliferation. We conclude that C6 cells and human glial tumour-cell lines were induced to differentiate by DSA.', 'During organogenesis in the central nervous system, differentiated astrocytes are characterised by a high expression of GFAP, a change from a polygonal shape to a stellate shape, and a low rate of proliferation. In the present study, we showed that DSA-treated cells express the characteristics of differentiation, i.e., they had a high content of GFAP, long processes, and a low rate of proliferation.', 'During organogenesis in the central nervous system, differentiated astrocytes are characterised by a high expression of GFAP, a change from a polygonal shape to a stellate shape, and a low rate of proliferation. In the present study, we showed that DSA-treated cells express the characteristics of differentiation, i.e., they had a high content of GFAP, long processes, and a low rate of proliferation. GFAP is expressed exclusively in astrocytes, and that the amount of GFAP is closely related to the differentiation of astrocytes.', 'In the present study, we showed that DSA-treated cells express the characteristics of differentiation, i.e., they had a high content of GFAP, long processes, and a low rate of proliferation. GFAP is expressed exclusively in astrocytes, and that the amount of GFAP is closely related to the differentiation of astrocytes. In addition to being a marker of matured astrocytes, GFAP is considered to be important for the induction and maintenance of astrocyte differentiation, because introduction of GFAP to C6 cells and human astrocytoma induced morphological changes, and suppressed their proliferation and invasive potential.', 'GFAP is expressed exclusively in astrocytes, and that the amount of GFAP is closely related to the differentiation of astrocytes. In addition to being a marker of matured astrocytes, GFAP is considered to be important for the induction and maintenance of astrocyte differentiation, because introduction of GFAP to C6 cells and human astrocytoma induced morphological changes, and suppressed their proliferation and invasive potential.', 'In summary, we showed that DSA induced differentiation and inhibited the proliferation of glioma cells. After addition of DSA to glioma cells, the cells grew at a much reduced rate, they changed from a flattened epithelioidal shape to a stellate shape having two or more long processes, so that some of the cells resembled normal fibrous astrocytes, and their content of GFAP was strikingly increased. These changes showed that DSA caused glioma cells to differentiate and thus may be useful as a new therapy for human glioma.', 'Regulation of glial fibrillary acidic protein (GFAP) expression in CNS development and in pathological states', 'Synthesis of glial fibrillary acidic protein in rat C6 glioma in chemically defined medium: cyclic AMP-dependent transcriptional and translational regulation', 'Transfection of human astrocytoma cells with glial fibrillary acidic protein complementary DNA: analysis of expression, proliferation, and tumorigenicity', 'Immunocytochemical study of the glial fibrillary acidic protein in human neoplasms of the central nervous system', 'Cell growth suppression of astrocytoma C6 cells by glial fibrillary acidic protein cDNA transfection']\n",
            "There are 21 occurrences (each are AT MOST 3-setences long SO MAX of 63 sentences in total) of 2670:glial fibrillary acidic protein.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install biopython\n",
        "import xml.etree.ElementTree as ET\n",
        "from Bio import Entrez\n",
        "import re\n",
        "import time\n",
        "\n",
        "class Gene:\n",
        "    def __init__(self, gene_id):\n",
        "        self.gene_id = gene_id\n",
        "        self.occurrences = []  # list to store snippet(s) where the gene was mentioned\n",
        "        self.symbol = None\n",
        "        self.organism = None\n",
        "        self.full_name = None\n",
        "        self.also_known_as = None\n",
        "        self.name_from_article = None\n",
        "        # TODO: add pmid/pmcid which will be used for final output. \n",
        "\n",
        "    def add_occurrence(self, snippet):\n",
        "        if snippet not in self.occurrences:  # Avoid duplicates\n",
        "            self.occurrences.append(snippet)\n",
        "\n",
        "    def set_name_from_article(self, name_from_article):\n",
        "        \"\"\"Sets the temporary name of the gene. This is the name accroding to the article\"\"\"\n",
        "        # will be used if the official name is not available\n",
        "        self.name_from_article = name_from_article\n",
        "\n",
        "    def get_name_from_article(self):\n",
        "        return self.name_from_article\n",
        "\n",
        "    def get_occurrences(self):\n",
        "        return self.occurrences\n",
        "\n",
        "    def get_also_known_as(self):\n",
        "        return self.also_known_as\n",
        "\n",
        "    def get_gene_id(self):\n",
        "        return self.gene_id\n",
        "\n",
        "    def update_info(self, symbol, organism, full_name, also_known_as):\n",
        "        self.symbol = symbol\n",
        "        self.organism = organism\n",
        "        self.full_name = full_name\n",
        "        self.also_known_as = also_known_as\n",
        "\n",
        "    def __repr__(self):\n",
        "        return (f\"Gene({self.gene_id})\\n\"\n",
        "                f\"  Symbol          : {self.symbol}\\n\"\n",
        "                f\"  Organism        : {self.organism}\\n\"\n",
        "                f\"  Full Name       : {self.full_name}\\n\"\n",
        "                f\"  Also Known As   : {self.also_known_as}\\n\"\n",
        "                f\"  In-text Name    : {self.name_from_article}\\n\"\n",
        "                f\"  Occurrences     : {self.occurrences}\")   # occurrences is a list of 3-sentence snippets\n",
        "\n",
        "def parse_xml_file(xml_path):\n",
        "    \"\"\"Parses the XML file and returns a gene dictionary keyed by gene ID.\"\"\"\n",
        "    tree = ET.parse(xml_path)\n",
        "    root = tree.getroot()\n",
        "    gene_dict = {}\n",
        "\n",
        "    for document in root.findall('document'):\n",
        "        for passage in document.findall('passage'):\n",
        "            section_type_elem = passage.find(\"infon[@key='section_type']\")\n",
        "            if section_type_elem is not None and (section_type_elem.text.upper() == \"METHODS\") or (section_type_elem.text.upper() == \"FIG\") or (section_type_elem.text.upper() == \"TABLE\"):\n",
        "                continue  # Skip passages under METHODS\n",
        "            # Get the full passage text.\n",
        "            passage_text_elem = passage.find(\"text\")\n",
        "            passage_text = passage_text_elem.text if passage_text_elem is not None else \"\"\n",
        "            # determine the starting offset for this passage.\n",
        "            passage_offset_elem = passage.find(\"offset\")\n",
        "            passage_offset = int(passage_offset_elem.text) if passage_offset_elem is not None else 0\n",
        "            # Split the passage into sentences using regex.\n",
        "            sentences = re.split(r'(?<=[.!?])\\s+', passage_text)\n",
        "            # Compute start indices for each sentence within the passage text.\n",
        "            start_indices = []\n",
        "            current_index = passage_offset\n",
        "            for sentence in sentences:\n",
        "                start_indices.append(current_index)\n",
        "                current_index += len(sentence) + 1  # account for the delimiter space\n",
        "\n",
        "            processed_ranges = set()  # To track which sentences have already been covered\n",
        "\n",
        "            # Process each annotation in the passage.\n",
        "            for annotation in passage.findall('annotation'):\n",
        "                ann_type = annotation.find(\"infon[@key='type']\")\n",
        "                if ann_type is not None and ann_type.text == \"Gene\":\n",
        "                    # Check for both 'identifier' and 'NCBI Gene' keys\n",
        "                    gene_id_elem = annotation.find(\"infon[@key='identifier']\")\n",
        "                    if gene_id_elem is None:\n",
        "                        gene_id_elem = annotation.find(\"infon[@key='NCBI Gene']\")\n",
        "                    gene_id = gene_id_elem.text if gene_id_elem is not None else None\n",
        "\n",
        "                    # Extract the gene name from the annotation text. (Temporary name if official name is not available)\n",
        "                    in_text_gene_name_elem = annotation.find(\"text\")\n",
        "                    in_text_gene_name = in_text_gene_name_elem.text if in_text_gene_name_elem is not None else None\n",
        "\n",
        "                    # Extract the annotation location (offset) to find the sentence containing the gene.\n",
        "                    location_elem = annotation.find(\"location\")\n",
        "                    ann_offset = int(location_elem.attrib.get('offset', 0)) if location_elem is not None else 0\n",
        "\n",
        "                    # Determine which sentence contains the annotation based on its offset.\n",
        "                    sentence_index = None\n",
        "                    for i, start in enumerate(start_indices):\n",
        "                        if start <= ann_offset < start + len(sentences[i]):\n",
        "                            sentence_index = i\n",
        "                            break\n",
        "\n",
        "                    # sentence_buffer\n",
        "                    if sentence_index is not None:\n",
        "                        start_sentence = max(0, sentence_index - 1)  # one sentence before\n",
        "                        end_sentence = min(len(sentences), sentence_index + 2)  # one sentence after\n",
        "                        range_tuple = (start_sentence, end_sentence)\n",
        "                        if range_tuple in processed_ranges:\n",
        "                            continue  # Skip duplicate extractions\n",
        "                        processed_ranges.add(range_tuple)\n",
        "                        snippet = \" \".join(sentences[start_sentence:end_sentence])\n",
        "                        if gene_id:\n",
        "                            if gene_id in gene_dict:\n",
        "                                gene_dict[gene_id].add_occurrence(snippet)\n",
        "                            else:\n",
        "                                gene_obj = Gene(gene_id)\n",
        "                                gene_obj.add_occurrence(snippet)\n",
        "                                gene_obj.set_name_from_article(in_text_gene_name)       # Temporary name if official name is not available\n",
        "                                gene_dict[gene_id] = gene_obj\n",
        "    return gene_dict\n",
        "\n",
        "def fetch_and_update_gene_info(gene_dict):\n",
        "    \"\"\"Retrieves gene information from NCBI and updates the genes in gene_dict.\"\"\"\n",
        "    gene_ids = list(gene_dict.keys())\n",
        "    if gene_ids:\n",
        "        # Post the gene IDs to NCBI.\n",
        "        handle = Entrez.epost(db=\"gene\", id=\",\".join(gene_ids))\n",
        "        result = Entrez.read(handle)\n",
        "        handle.close()\n",
        "\n",
        "        webenv = result[\"WebEnv\"]\n",
        "        query_key = result[\"QueryKey\"]\n",
        "\n",
        "        handle = Entrez.esummary(db=\"gene\", webenv=webenv, query_key=query_key)\n",
        "        record = Entrez.read(handle)\n",
        "        handle.close()\n",
        "\n",
        "        for docsum in record[\"DocumentSummarySet\"][\"DocumentSummary\"]:\n",
        "            gene_id = docsum.attributes[\"uid\"]\n",
        "            symbol = docsum.get('NomenclatureSymbol', 'No symbol')\n",
        "            organism = docsum.get('Organism', {}).get('ScientificName', 'No organism')\n",
        "            full_name = docsum.get('NomenclatureName', gene_dict[gene_id].get_name_from_article())\n",
        "            also_known_as = docsum.get('OtherAliases', gene_dict[gene_id].get_name_from_article())\n",
        "\n",
        "            # if full name and also known as are not available, use the name from the article\n",
        "            if (full_name == ''):\n",
        "                full_name = gene_dict[gene_id].get_name_from_article()\n",
        "            if (also_known_as == ''):\n",
        "                also_known_as = gene_dict[gene_id].get_name_from_article()\n",
        "\n",
        "            if gene_id in gene_dict:\n",
        "                gene_dict[gene_id].update_info(symbol, organism, full_name, also_known_as)\n",
        "            # Pause briefly to avoid overwhelming NCBI servers.\n",
        "            time.sleep(0.5)     # 2 requests per second (safe). WITH API KEY, be increased if needed to 10 requests per second.\n",
        "\n",
        "\n",
        "# Set your email (and API key if available)\n",
        "Entrez.email = \"email here\"\n",
        "# Entrez.api_key = \"your_api_key\"\n",
        "\n",
        "# example2.xml takes a while since it has 64 genes.\n",
        "xml_path = \"/content/full_text_annotated_example.xml\"\n",
        "gene_dict = parse_xml_file(xml_path)\n",
        "fetch_and_update_gene_info(gene_dict)           # Fetch gene information from NCBI (Optional but has good information)\n",
        "for gene in gene_dict.values():\n",
        "    print(gene)\n",
        "    print(f\"There are {len(gene.get_occurrences())} occurrences (each are AT MOST 3-setences long SO MAX of {int(3*len(gene.get_occurrences()))} sentences in total) of {gene.get_gene_id()}:{gene.get_name_from_article()}.\")\n",
        "    print(\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33A7yYBLXZes"
      },
      "source": [
        "# 3.5 Prompt Engineering\n",
        "- Creates the prompt using the Gene dictionary (from Gene chunking)\n",
        "- batch_size: allows you to have more than 1 Gene in the prompt such that you have something like the following. Ideally keep it at batch_size=1 to ensure that the GO terms generated are exclusive to the Gene being analyzed.\n",
        "\n",
        "```\n",
        "# batch_size > 1 has the basic format:\n",
        "Basic Prompt\n",
        "Gene 1\n",
        "Gene 2\n",
        "...\n",
        "# batch_size = 1 has the basic format:\n",
        "Basic Prompt\n",
        "Gene 1\n",
        "```\n",
        "- batch_index: allows you to iterate through the gene dictionary so that if you have a paper with x amount of genes, you can just increment  the index and run the inference x amount of times.\n",
        "- direct: ignore this. This is for the future.\n",
        "- customized_prompt: ignore this as well. This is for the future."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsRz3Wb5YTQl",
        "outputId": "c8782998-3150-4d93-c2d4-64f9671a2780"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Analyze the following gene information and text snippets under \"Occurrences\".\n",
            "Identify all potentially relevant Gene Ontology (GO) terms.\n",
            "Base your analysis on prior knowledge available in your training data.\n",
            "\n",
            "After completing your analysis, for each Gene Ontology (GO) term you list, provide a short one-line justification referencing the text as well as a relationship between the gene and the term.\n",
            "If you cannot identify any GO terms for a particular gene information, say “None found.”\n",
            "\n",
            "Be concise, do not use unnecessary words.\n",
            "Be factual, do not editorialize.\n",
            "Be specific, avoid overly general statements like \"it is involved in many cellular processes.\n",
            "\n",
            "To help you in your work, I am providing an example of gene information and the corresponding example analysis output structured in JSON.\n",
            "\n",
            "Example gene information is:\n",
            "Gene: myogenin\n",
            "Organism: Mus musculus\n",
            "Occurrences:\n",
            "- Activation of NF-kappaB increases the expression of the inducible nitric oxide synthase (iNOS) in muscle cells that sequester HuR (RNA-binding protein) by preventing transcription of MyoD. Denervation-induced atrophy is related to the upregulation of specific histone deacetylases (HDAC), able to repress a negative regulator of myogenin, with an increase in MuRF1 expression and muscle wasting. AMPK (protein kinase activated by 5'adenosine monophosphate) is also involved in muscle atrophy.\n",
            "\n",
            "Example anaylysis output is:\n",
            "[\n",
            "  {\n",
            "    \"gene\": \"myogenin\",\n",
            "    \"go_data\": [\n",
            "      {\n",
            "        \"term\": \"DNA-binding transcription activator activity\",\n",
            "        \"relationship\": \"enables\",\n",
            "        \"namespace\": \"molecular_function\",\n",
            "        \"justification\": \"Myogenin is a key transcription factor in muscle differentiation that binds to E-box sequences in muscle-specific genes and activates their transcription through RNA polymerase II.\"\n",
            "\n",
            "      },\n",
            "      {\n",
            "        \"term\": \"skeletal muscle cell differentiation\",\n",
            "        \"relationship\": \"involved_in\",\n",
            "        \"namespace\": \"biological_process\"\n",
            "        \"justification\": \"Myogenin is one of the four myogenic regulatory factors (MRFs) that drive skeletal muscle differentiation by promoting the transition from myoblasts to myotubes.\"\n",
            "      }]\n",
            "    }\n",
            "]\n",
            "\n",
            "Here is the gene information:\n",
            "\n",
            "Gene: glial fibrillary acidic protein\n",
            "Occurrences:\n",
            "- We found that a lectin, Datura stramonium agglutinin, induced irreversible differentiation in C6 glioma cells. The differentiated cells had long processes, a low rate of proliferation and a high content of glial fibrillary acidic protein. When the medium was replaced with Datura stramonium agglutinin-free medium after 1 h, cell proliferation continued to be inhibited.\n",
            "- Proliferation of four human glial tumour cells was also inhibited by Datura stramonium agglutinin. Further, these differentiated human glial tumour cells had long processes and a high content of glial fibrillary acidic protein similar to differentiated C6 glioma cells. Taken together, these observations suggest that Datura stramonium agglutinin may be useful as a new therapy for treating glioma without side effects.\n",
            "- Recently, we found that DSA induced differentiation of astrocytes. Addition of DSA caused a morphological change from a polygonal shape to a stellate shape with many long processes, an increase of glial fibrillary acidic protein (GFAP) and suppression of proliferation. Therefore, it was of interest to determine whether DSA can inhibit cell-specific proliferation and induce differentiation of glioma cells.\n",
            "- Increased expression of GFAP after addition of DSA\n",
            "- The malignancy of gliomas is inversely correlated with the content of astrocyte-specific intermediate filament protein (GFAP). Furthermore, mature astrocytes are characterised by the expression of a large amount of GFAP.\n",
            "- The malignancy of gliomas is inversely correlated with the content of astrocyte-specific intermediate filament protein (GFAP). Furthermore, mature astrocytes are characterised by the expression of a large amount of GFAP. GFAP is expressed exclusively in astrocytes and the expression level increases during differentiation.\n",
            "- Furthermore, mature astrocytes are characterised by the expression of a large amount of GFAP. GFAP is expressed exclusively in astrocytes and the expression level increases during differentiation. C6 cells were found to produce only a small amount of GFAP due to their malignant properties.\n",
            "- GFAP is expressed exclusively in astrocytes and the expression level increases during differentiation. C6 cells were found to produce only a small amount of GFAP due to their malignant properties. The immunoblot in Figure 3 shows clearly that the expression of GFAP was enhanced after the addition of DSA.\n",
            "- C6 cells were found to produce only a small amount of GFAP due to their malignant properties. The immunoblot in Figure 3 shows clearly that the expression of GFAP was enhanced after the addition of DSA. A densitometric evaluation of the blot indicated that the amount of GFAP after the addition of DSA increased over hundred fold relative to the amount in the control cells.\n",
            "- The immunoblot in Figure 3 shows clearly that the expression of GFAP was enhanced after the addition of DSA. A densitometric evaluation of the blot indicated that the amount of GFAP after the addition of DSA increased over hundred fold relative to the amount in the control cells. The corresponding values of human glial tumour cell lines were more than 12.2 times.\n",
            "- The corresponding values of human glial tumour cell lines were more than 12.2 times. Differentiated astrocytes are characterised by a stellate cell morphology, increased GFAP expression, and inhibited proliferation. We conclude that C6 cells and human glial tumour-cell lines were induced to differentiate by DSA.\n",
            "- During organogenesis in the central nervous system, differentiated astrocytes are characterised by a high expression of GFAP, a change from a polygonal shape to a stellate shape, and a low rate of proliferation. In the present study, we showed that DSA-treated cells express the characteristics of differentiation, i.e., they had a high content of GFAP, long processes, and a low rate of proliferation.\n",
            "- During organogenesis in the central nervous system, differentiated astrocytes are characterised by a high expression of GFAP, a change from a polygonal shape to a stellate shape, and a low rate of proliferation. In the present study, we showed that DSA-treated cells express the characteristics of differentiation, i.e., they had a high content of GFAP, long processes, and a low rate of proliferation. GFAP is expressed exclusively in astrocytes, and that the amount of GFAP is closely related to the differentiation of astrocytes.\n",
            "- In the present study, we showed that DSA-treated cells express the characteristics of differentiation, i.e., they had a high content of GFAP, long processes, and a low rate of proliferation. GFAP is expressed exclusively in astrocytes, and that the amount of GFAP is closely related to the differentiation of astrocytes. In addition to being a marker of matured astrocytes, GFAP is considered to be important for the induction and maintenance of astrocyte differentiation, because introduction of GFAP to C6 cells and human astrocytoma induced morphological changes, and suppressed their proliferation and invasive potential.\n",
            "- GFAP is expressed exclusively in astrocytes, and that the amount of GFAP is closely related to the differentiation of astrocytes. In addition to being a marker of matured astrocytes, GFAP is considered to be important for the induction and maintenance of astrocyte differentiation, because introduction of GFAP to C6 cells and human astrocytoma induced morphological changes, and suppressed their proliferation and invasive potential.\n",
            "- In summary, we showed that DSA induced differentiation and inhibited the proliferation of glioma cells. After addition of DSA to glioma cells, the cells grew at a much reduced rate, they changed from a flattened epithelioidal shape to a stellate shape having two or more long processes, so that some of the cells resembled normal fibrous astrocytes, and their content of GFAP was strikingly increased. These changes showed that DSA caused glioma cells to differentiate and thus may be useful as a new therapy for human glioma.\n",
            "- Regulation of glial fibrillary acidic protein (GFAP) expression in CNS development and in pathological states\n",
            "- Synthesis of glial fibrillary acidic protein in rat C6 glioma in chemically defined medium: cyclic AMP-dependent transcriptional and translational regulation\n",
            "- Transfection of human astrocytoma cells with glial fibrillary acidic protein complementary DNA: analysis of expression, proliferation, and tumorigenicity\n",
            "- Immunocytochemical study of the glial fibrillary acidic protein in human neoplasms of the central nervous system\n",
            "- Cell growth suppression of astrocytoma C6 cells by glial fibrillary acidic protein cDNA transfection\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def make_go_term_prompt(gene_dict, batch_size=1, batch_index=0, direct=False, customized_prompt=None):\n",
        "    \"\"\"\n",
        "    Create a prompt for Llama to output Gene Ontology (GO) terms (without IDs) for a batch of genes\n",
        "    from the gene dictionary. The output is expected to be categorized into three sections for each gene:\n",
        "      1. Biological Processes\n",
        "      2. Cellular Components\n",
        "      3. Molecular Functions\n",
        "\n",
        "    For each gene, the output should follow the format below without mixing results across different genes.\n",
        "\n",
        "    :param gene_dict: A dictionary where each key is a gene ID and each value is a Gene object that\n",
        "                      contains attributes such as symbol, organism, full_name, and occurrences.\n",
        "    :param batch_size: The number of genes to include in this prompt batch.\n",
        "    :param batch_index: The index (0-based) of the batch to process. For instance, if batch_size=1,\n",
        "                        batch_index=2 will include the 3rd gene in the dictionary.\n",
        "    :param direct: If True, use a more direct instruction for GO term extraction.\n",
        "    :param customized_prompt: If provided, use this custom prompt text in place of the default.\n",
        "    :return: A string containing the complete prompt.\n",
        "    \"\"\"\n",
        "\n",
        "    # Instruction blocks\n",
        "\n",
        "    # context = \"\"\"You are an efficient and insightful assistant to a geneticist.\"\"\"\n",
        "\n",
        "    general_instructions = \"\"\"Analyze the following gene information and text snippets under \"Occurrences\".\n",
        "Identify all potentially relevant Gene Ontology (GO) terms.\n",
        "Base your analysis on prior knowledge available in your training data.\n",
        "\n",
        "After completing your analysis, for each Gene Ontology (GO) term you list, provide a short one-line justification referencing the text as well as a relationship between the gene and the term.\n",
        "If you cannot identify any GO terms for a particular gene information, say “None found.”\n",
        "\n",
        "Be concise, do not use unnecessary words.\n",
        "Be factual, do not editorialize.\n",
        "Be specific, avoid overly general statements like \"it is involved in many cellular processes.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "    example = \"\"\"To help you in your work, I am providing an example of gene information and the corresponding example analysis output structured in JSON.\n",
        "\n",
        "Example gene information is:\n",
        "Gene: myogenin\n",
        "Organism: Mus musculus\n",
        "Occurrences:\n",
        "- Activation of NF-kappaB increases the expression of the inducible nitric oxide synthase (iNOS) in muscle cells that sequester HuR (RNA-binding protein) by preventing transcription of MyoD. Denervation-induced atrophy is related to the upregulation of specific histone deacetylases (HDAC), able to repress a negative regulator of myogenin, with an increase in MuRF1 expression and muscle wasting. AMPK (protein kinase activated by 5'adenosine monophosphate) is also involved in muscle atrophy.\n",
        "\n",
        "Example anaylysis output is:\n",
        "[\n",
        "  {\n",
        "    \"gene\": \"myogenin\",\n",
        "    \"go_data\": [\n",
        "      {\n",
        "        \"term\": \"DNA-binding transcription activator activity\",\n",
        "        \"relationship\": \"enables\",\n",
        "        \"namespace\": \"molecular_function\",\n",
        "        \"justification\": \"Myogenin is a key transcription factor in muscle differentiation that binds to E-box sequences in muscle-specific genes and activates their transcription through RNA polymerase II.\"\n",
        "\n",
        "      },\n",
        "      {\n",
        "        \"term\": \"skeletal muscle cell differentiation\",\n",
        "        \"relationship\": \"involved_in\",\n",
        "        \"namespace\": \"biological_process\"\n",
        "        \"justification\": \"Myogenin is one of the four myogenic regulatory factors (MRFs) that drive skeletal muscle differentiation by promoting the transition from myoblasts to myotubes.\"\n",
        "      }]\n",
        "    }\n",
        "]\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "    # Assemble the prompt text based on the chosen mode\n",
        "    if direct:\n",
        "        # prompt_text = context\n",
        "        # prompt_text += direct_instructions\n",
        "        # prompt_text += format_instructions\n",
        "        # prompt_text += example_output\n",
        "        pass\n",
        "    elif customized_prompt:\n",
        "        # prompt_text = context\n",
        "        # prompt_text += customized_prompt\n",
        "        # prompt_text += format_instructions\n",
        "        # prompt_text += example_output\n",
        "        pass\n",
        "    else:\n",
        "        prompt_text = general_instructions\n",
        "        prompt_text += example\n",
        "        prompt_text += \"\\nHere is the gene information:\\n\"\n",
        "\n",
        "    # Convert the gene dictionary to a list and determine the batch slice.\n",
        "    gene_items = list(gene_dict.items())\n",
        "    start_index = batch_index * batch_size\n",
        "    end_index = start_index + batch_size\n",
        "    batch_genes = gene_items[start_index:end_index]\n",
        "\n",
        "    # Add details and occurrence context for each gene in the selected batch.\n",
        "    for gene_id, gene_obj in batch_genes:\n",
        "        # prompt_text += f\"\\nGene ID: {gene_id}\\n\"\n",
        "        # prompt_text += f\"Symbol: {gene_obj.symbol}\\n\"\n",
        "        # prompt_text += f\"Organism: {gene_obj.organism}\\n\"\n",
        "        # prompt_text += f\"Full Name: {gene_obj.full_name}\\n\"\n",
        "        prompt_text += f\"\\nGene: {gene_obj.get_name_from_article()}\\n\"\n",
        "        prompt_text += \"Occurrences:\\n\"\n",
        "        for occ in gene_obj.occurrences:\n",
        "            prompt_text += f\"- {occ}\\n\"\n",
        "\n",
        "    return prompt_text\n",
        "\n",
        "# Example usage:\n",
        "# Assuming gene_dict is defined and contains Gene objects with attributes: symbol, organism, full_name, and occurrences.\n",
        "# To process only one gene at a time:\n",
        "# prompt = make_go_term_prompt(gene_dict, batch_size=1, batch_index=0)\n",
        "# print(prompt)\n",
        "\n",
        "prompt = make_go_term_prompt(gene_dict)\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oaKSo6iXGOc"
      },
      "source": [
        "# 4. Inference (Local)\n",
        "The two cells below rely on local inference meaning that you must have the model locally. Skip them if youre using the API like GROQ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "hzFESXHRXBHG",
        "outputId": "11a35c74-8f63-4af1-8290-454182d1058e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading LLaMA model.\n",
            "False\n"
          ]
        },
        {
          "ename": "OSError",
          "evalue": "Incorrect path_or_model_id: '/content/drive/MyDrive/GOLLM/Llama 3.2-3B-Instruct-model'. Please provide either the path to a local folder or the repo_id of a model on the Hub.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    404\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"repo_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"from_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"to_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mvalidate_repo_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrepo_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         raise HFValidationError(\n\u001b[0m\u001b[1;32m    155\u001b[0m             \u001b[0;34m\"Repo id must be in the form 'repo_name' or 'namespace/repo_name':\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/content/drive/MyDrive/GOLLM/Llama 3.2-3B-Instruct-model'. Use `repo_type` argument if needed.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-2a02598bcb50>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m         \u001b[0;31m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m         \u001b[0mtokenizer_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tokenizer_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"_commit_hash\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenizer_config\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mget_tokenizer_config\u001b[0;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m     \u001b[0mcommit_hash\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m     resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m    704\u001b[0m         \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0mTOKENIZER_CONFIG_FILE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"There was a specific connection error when trying to load {path_or_repo_id}:\\n{err}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHFValidationError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m         raise EnvironmentError(\n\u001b[0m\u001b[1;32m    470\u001b[0m             \u001b[0;34mf\"Incorrect path_or_model_id: '{path_or_repo_id}'. Please provide either the path to a local folder or the repo_id of a model on the Hub.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m         ) from e\n",
            "\u001b[0;31mOSError\u001b[0m: Incorrect path_or_model_id: '/content/drive/MyDrive/GOLLM/Llama 3.2-3B-Instruct-model'. Please provide either the path to a local folder or the repo_id of a model on the Hub."
          ]
        }
      ],
      "source": [
        "print(\"Loading LLaMA model.\")\n",
        "# model_dir = \"/content/drive/MyDrive/Llama 3.2-3B-Instruct-model\"\n",
        "model_dir = \"/content/drive/MyDrive/GOLLM/Llama 3.2-3B-Instruct-model\"\n",
        "device = torch.device(\"cuda\")\n",
        "print(torch.cuda.is_available())\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_dir, local_files_only=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_dir, local_files_only=True).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "IzBYJajjYYly",
        "outputId": "6bb45d42-3202-4076-aa8e-e40a15c356e1"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'test_prompt' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-6470a21189ee>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_prompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'test_prompt' is not defined"
          ]
        }
      ],
      "source": [
        "def run_inference(prompt):\n",
        "    \"\"\"\n",
        "    Run inference on the provided prompt using the specified model and tokenizer.\n",
        "\n",
        "    :param prompt: The prompt string to send to the model.\n",
        "    :return: The generated text.\n",
        "    \"\"\"\n",
        "\n",
        "    # Encode the prompt and move to the appropriate device\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    attention_mask = inputs[\"attention_mask\"]\n",
        "\n",
        "    # Greedy or minimal variation == When: You want consistent, short answers, and you don’t mind if it’s a bit “blunt.”\n",
        "    # outputs = model.generate(\n",
        "    #     input_ids=inputs[\"input_ids\"],\n",
        "    #     attention_mask=attention_mask,\n",
        "    #     max_new_tokens=150,\n",
        "    #     do_sample=False,     # no sampling\n",
        "    #     num_beams=1,         # purely greedy\n",
        "    #     no_repeat_ngram_size=3,\n",
        "    #     pad_token_id=tokenizer.eos_token_id\n",
        "    # )\n",
        "\n",
        "    # Beam Search (Higher Quality / Less Randomness) == You want a more “global optimum” text.\n",
        "    # outputs = model.generate(\n",
        "    #     input_ids=inputs[\"input_ids\"],\n",
        "    #     attention_mask=attention_mask,\n",
        "    #     max_new_tokens=200,\n",
        "    #     do_sample=False,\n",
        "    #     num_beams=4,      # search multiple beams\n",
        "    #     length_penalty=1.0,  # see if you want to encourage or discourage long outputs\n",
        "    #     no_repeat_ngram_size=3,\n",
        "    #     pad_token_id=tokenizer.eos_token_id\n",
        "    # )\n",
        "\n",
        "    # Beam SearchV2\n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        attention_mask=attention_mask,\n",
        "        max_new_tokens=400,       # Enough tokens for a concise but thorough answer\n",
        "        do_sample=False,          # Turn off sampling; we want a more deterministic, stable answer\n",
        "        num_beams=3,              # Explore multiple beams for higher-quality completions\n",
        "        length_penalty=0.1,       # 1.0 means \"neutral\" length preference (>=1.0 encourages longer outputs)\n",
        "        no_repeat_ngram_size=4,   # Helps avoid repeating the same phrase\n",
        "        early_stopping=True,      # Stops as soon as the best beam is complete\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        temperature=None,\n",
        "        top_p=None,\n",
        "    )\n",
        "    # Sampling (More Creative / Less Deterministic)\n",
        "    # outputs = model.generate(\n",
        "    #     input_ids=inputs[\"input_ids\"],\n",
        "    #     attention_mask=attention_mask,\n",
        "    #     max_new_tokens=200,\n",
        "    #     do_sample=True,       # sampling\n",
        "    #     temperature=0.7,      # moderate creativity\n",
        "    #     top_p=0.9,            # nucleus sampling\n",
        "    #     no_repeat_ngram_size=3,\n",
        "    #     pad_token_id=tokenizer.eos_token_id\n",
        "    # )\n",
        "\n",
        "    # Decode the output and return the generated text\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    completion = generated_text[len(prompt):].strip()\n",
        "    return completion\n",
        "\n",
        "for i in range(3):\n",
        "    start_time = time.time()\n",
        "    output = run_inference(test_prompt)\n",
        "    print(output)\n",
        "\n",
        "    # with open(f'/content/drive/MyDrive/GOLLM/Inference Outputs/output_{i+1}.txt', 'w') as file:\n",
        "    #     file.write(output)\n",
        "\n",
        "    print(f\"-->Model loaded on {device} named {torch.cuda.get_device_name(0)} and it took {time.time()-start_time:.2f} seconds.\")\n",
        "    print(\"-\"*50)\n",
        "\n",
        "# prompt = \"what is 2+2\"\n",
        "# print(\"Running inference...\")\n",
        "# start_time = time.time()\n",
        "# output = run_inference(prompt)\n",
        "# print(f\"Model loaded on {device} in {time.time()-start_time:.2f} seconds.\")\n",
        "# print(output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSz3gKoaY7yu"
      },
      "source": [
        "# 4v2. GROQ API Inference\n",
        "- need API Key\n",
        "- uses llama 3.3-70b-versatile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_k_yNbOwpkb",
        "outputId": "e835d56a-34ec-4829-aa07-42e25f7cd240"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: Groq in /usr/local/lib/python3.11/dist-packages (0.18.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from Groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from Groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from Groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from Groq) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from Groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from Groq) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->Groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->Groq) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->Groq) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->Groq) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->Groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->Groq) (2.27.2)\n"
          ]
        }
      ],
      "source": [
        "# using GROQ API\n",
        "!pip install Groq\n",
        "from groq import Groq\n",
        "\n",
        "client = Groq(\n",
        "    api_key=\"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\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eOnOkZsztbK",
        "outputId": "1b6bda1e-0db8-4e2d-d031-3d2d33500902"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "   \"gene\":\"glial fibrillary acidic protein\",\n",
            "   \"go_data\":[\n",
            "      {\n",
            "         \"term\":\"intermediate filament\",\n",
            "         \"relationship\":\"enables\",\n",
            "         \"namespace\":\"cellular_component\",\n",
            "         \"justification\":\"GFAP is expressed exclusively in astrocytes and is a type of intermediate filament protein.\"\n",
            "      },\n",
            "      {\n",
            "         \"term\":\"astrocyte differentiation\",\n",
            "         \"relationship\":\"involved_in\",\n",
            "         \"namespace\":\"biological_process\",\n",
            "         \"justification\":\"GFAP is considered to be important for the induction and maintenance of astrocyte differentiation.\"\n",
            "      },\n",
            "      {\n",
            "         \"term\":\"regulation of cell proliferation\",\n",
            "         \"relationship\":\"involved_in\",\n",
            "         \"namespace\":\"biological_process\",\n",
            "         \"justification\":\"Introduction of GFAP to C6 cells and human astrocytoma induced morphological changes, and suppressed their proliferation.\"\n",
            "      },\n",
            "      {\n",
            "         \"term\":\"cytoskeleton organization\",\n",
            "         \"relationship\":\"involved_in\",\n",
            "         \"namespace\":\"biological_process\",\n",
            "         \"justification\":\"GFAP is a type of intermediate filament protein that provides structural support to astrocytes.\"\n",
            "      },\n",
            "      {\n",
            "         \"term\":\"central nervous system development\",\n",
            "         \"relationship\":\"involved_in\",\n",
            "         \"namespace\":\"biological_process\",\n",
            "         \"justification\":\"GFAP is expressed during organogenesis in the central nervous system and is involved in the development of astrocytes.\"\n",
            "      }\n",
            "   ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# Generate the prompt for the gene (ensure gene_dict is defined)\n",
        "prompt = make_go_term_prompt(gene_dict, batch_index=0)  # remember to increment the index for other genes\n",
        "\n",
        "# Request a completion for gene analysis\n",
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt,\n",
        "        }\n",
        "    ],\n",
        "    model=\"llama-3.3-70b-versatile\",\n",
        "    temperature=0,\n",
        "    max_completion_tokens=2048,\n",
        "    top_p=0.5,\n",
        "    stream=False,\n",
        "    response_format={\"type\": \"json_object\"},\n",
        "    stop=None,\n",
        ")\n",
        "\n",
        "# Save the output in a variable\n",
        "llama_output = chat_completion.choices[0].message.content\n",
        "\n",
        "# Print the output from the model\n",
        "print(llama_output)\n",
        "\n",
        "# Optionally, write the output to a JSON file for later use\n",
        "with open(\"llama_output.json\", \"w\") as f:\n",
        "    f.write(llama_output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P58Ns1lwdbbg"
      },
      "source": [
        "# 5. Normalize\n",
        "- use an embedding model (SAPBERT) to ensure that the terms generated by Llama are actual GO terms. You will need the Go.obo file from the [Gene Ontology Consortium](https://geneontology.org/docs/download-ontology/)\n",
        "  - Essentially, for each go term created by Llama, Sapbert will do a similarity search against the Go.obo and retrieve the actual term + GO:ID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5lKdeAZ1Ssh",
        "outputId": "5f90a7bb-b607-4d81-878b-ae6cb51bb028"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.48.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.5.1+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.28.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.5)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m89.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Collecting obonet\n",
            "  Downloading obonet-1.1.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from obonet) (3.4.2)\n",
            "Downloading obonet-1.1.0-py3-none-any.whl (9.1 kB)\n",
            "Installing collected packages: obonet\n",
            "Successfully installed obonet-1.1.0\n"
          ]
        }
      ],
      "source": [
        "# Install required packages (run these in your environment if not already installed)\n",
        "!pip install sentence-transformers\n",
        "!pip install obonet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TPMPZKF1P1X",
        "outputId": "2019bfdf-070b-4022-f845-cadb1cf5d9ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 40267 GO terms from /content/drive/MyDrive/Colab Notebooks/go.obo\n"
          ]
        }
      ],
      "source": [
        "import obonet\n",
        "import json\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import numpy as np\n",
        "\n",
        "##########################################\n",
        "# 1. Load and parse the GO.obo file\n",
        "##########################################\n",
        "\n",
        "def load_go_ontology(obo_filepath):\n",
        "    \"\"\"\n",
        "    Parse the GO.obo file using obonet.\n",
        "    Returns a dictionary mapping GO IDs to their term names.\n",
        "    \"\"\"\n",
        "    graph = obonet.read_obo(obo_filepath)\n",
        "    go_terms = {}\n",
        "    for go_id, data in graph.nodes(data=True):\n",
        "        # Only include nodes that have a 'name' field\n",
        "        if 'name' in data:\n",
        "            go_terms[go_id] = data\n",
        "    return go_terms\n",
        "\n",
        "# set the path to your local go.obo (or something else later) file\n",
        "obo_file = \"/content/drive/MyDrive/Colab Notebooks/go.obo\"\n",
        "go_terms_dict = load_go_ontology(obo_file)\n",
        "print(f\"Loaded {len(go_terms_dict)} GO terms from {obo_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340,
          "referenced_widgets": [
            "f7d1ab90bc2f4b29a9a5e742a0e35e28",
            "f6baa2c31fb44fa5bfdeb94123fcf496",
            "657a5a1cd09b4ba1a5ba5992b1ee128b",
            "cce594e541324d27909cf970d4db170c",
            "0b3cdc367d354839ba4b3aa843be3ae5",
            "e6c69cb9aa114f5c918981f26790bbf5",
            "7111a86c1d7c47daa9fd93001a902020",
            "cda625c7fcad401488ff4b6c0c0a5777",
            "bd339b4ef33448a8a9837ca47b8fc8f5",
            "558d35b72a8f477ea9cd83e5ce7f2552",
            "8ac0c98ff232418aa7d417aea5495fb0",
            "9e2b233314004ba791d7df6cd70c21a3",
            "4d3d0d4c1b9f4dac9b95ea254b6f4333",
            "627ec98728254f5eb3cd7fa2756dc113",
            "7ef8ea0dcca64b19bfd01d6c977ce0e8",
            "200bcf0b84634d568d89319d3828a012",
            "bc7be00627af4721a9e8686017e772f8",
            "10ff22b530bb4a609c9926814c0745f7",
            "54da763ba0714288975cafc0cd43a1f7",
            "9a98603d5ce74808a3c4069a447c50b6",
            "e4f0d71a138c4efc95593bcb7842be76",
            "1b30cf6534d5419380bb73aa2e4e6eae",
            "df1d5475258c4e3db84b8d2a16022961",
            "3bad79220be54a87a02cdbac6c6c8bf8",
            "e3d3dac2784c4841b41ab7d0fc982155",
            "70e32991c01148c98d4dc22a43d8c965",
            "5e8a3f461e0b421e8095b0c5c777e360",
            "61616813f2f148e296978d5db567b3a7",
            "4273a1a31d7a4caf86e16587eca40af5",
            "8d049df4c983441d82ca8bfe855abe67",
            "5f0fb5433a01427b992d132b7b6debf3",
            "6a10408f822e4bddbf31cca55a029bc4",
            "8a1f9a797f1a4ffe8bfef4e72bdfa280",
            "e0269ea1bc8441c5b8cf44485569ec08",
            "d99fc6f064ba439e9c00af09a2593416",
            "9d924d12256b4fd4802c28e035cc9967",
            "50fe181f565643a7b31e8c4baf67366a",
            "c7f16613bb3941beb112cff932eafef7",
            "239d1ef8640a40a8a043b171d7e3462f",
            "91b58c570960466491693c617aecb809",
            "f65573653f834d199bd6a300ce9fa4d1",
            "25ac4acfb3b242358c24fcff69447b1d",
            "34fd89eff8534c5ab156b9973e2374cc",
            "0a2df4b0204043feac1ab9887878b3c8",
            "8cce6c61b8864b0fa055237abe08006e",
            "f01cb77ea90c4b2bb4821c13e3b56ed6",
            "8ed8bc76be6a477699c9ab641493f82b",
            "21ffdc6e9de147128fa39f9cd58d7995",
            "1fec01a7236d4cf3acad0bd0c2dd402e",
            "3d3efcf524b74d08b559c2dad2ea52d2",
            "3c258adebdb04a2d98457fa7278a917a",
            "7fd41e8255bb4907ad2775eb6cb5f7d5",
            "077e993516bb4e68ae51101f6b9c2530",
            "3bc3505975e64aaf9bf96e65770de232",
            "b8cebd6679054a39abbaca928d63dbd0"
          ]
        },
        "id": "zfuUI3PF1boL",
        "outputId": "d98e82a0-9f59-4492-eef9-5a713165e7b1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name cambridgeltl/SapBERT-from-PubMedBERT-fulltext. Creating a new one with mean pooling.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f7d1ab90bc2f4b29a9a5e742a0e35e28",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/462 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9e2b233314004ba791d7df6cd70c21a3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "df1d5475258c4e3db84b8d2a16022961",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/198 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e0269ea1bc8441c5b8cf44485569ec08",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/226k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8cce6c61b8864b0fa055237abe08006e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded embedding model: cambridgeltl/SapBERT-from-PubMedBERT-fulltext\n"
          ]
        }
      ],
      "source": [
        "##########################################\n",
        "# 2. Load the SAPBERT embedding model (or an alternative)\n",
        "##########################################\n",
        "\n",
        "# Here we use a SAPBERT model available from Hugging Face.\n",
        "# may choose another model (e.g., \"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "model_name = \"cambridgeltl/SapBERT-from-PubMedBERT-fulltext\"  # SAPBERT model fine-tuned on UMLS\n",
        "sapbert_model = SentenceTransformer(model_name)\n",
        "print(f\"Loaded embedding model: {model_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OM1So4sX2JLT",
        "outputId": "54d2ace3-094c-405f-a48d-a3d12d898147"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedded GO terms with additional metadata.\n"
          ]
        }
      ],
      "source": [
        "def embed_go_terms(go_terms, model):\n",
        "    \"\"\"\n",
        "    Given a dictionary of GO terms (GO ID -> term data) and an embedding model,\n",
        "    returns:\n",
        "      - a list of GO IDs,\n",
        "      - a list of GO term names (in the same order),\n",
        "      - a tensor of embeddings for the GO term names,\n",
        "      - a list of GO namespaces,\n",
        "      - a list of GO relationships (if available, otherwise default to 'unknown')\n",
        "    \"\"\"\n",
        "    go_ids = []\n",
        "    go_names = []\n",
        "    go_namespaces = []\n",
        "    go_relationships = []\n",
        "\n",
        "    for go_id, data in go_terms.items():\n",
        "        if 'name' in data:\n",
        "            go_ids.append(go_id)\n",
        "            go_names.append(data['name'])\n",
        "            go_namespaces.append(data.get('namespace', 'unknown'))\n",
        "            # Some GO entries might not have a \"relationship\" field; default to 'unknown'\n",
        "            go_relationships.append(data.get('relationship', 'unknown'))\n",
        "\n",
        "    go_embeddings = model.encode(go_names, convert_to_tensor=True)\n",
        "    return go_ids, go_names, go_embeddings, go_namespaces, go_relationships\n",
        "\n",
        "# Reload GO ontology using the updated load_go_ontology:\n",
        "go_terms_dict = load_go_ontology(obo_file)\n",
        "go_ids, go_names, go_embeddings, go_namespaces, go_relationships = embed_go_terms(go_terms_dict, sapbert_model)\n",
        "print(\"Embedded GO terms with additional metadata.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0yR2eF6228J",
        "outputId": "42d0ce6a-6856-45ce-f341-428b06c70eef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example: Llama term 'muscle cell differentiation' matched with 'muscle cell differentiation' (GO:0042692) with score 1.00\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "def get_best_matching_go_term(llama_term, go_ids, go_names, go_embeddings, model, threshold=0.6):\n",
        "    \"\"\"\n",
        "    Given a GO term from Llama, compute its embedding and find the best matching GO term\n",
        "    from the provided GO ontology embeddings.\n",
        "\n",
        "    Parameters:\n",
        "      - llama_term: The GO term string generated by Llama.\n",
        "      - go_ids, go_names, go_embeddings: Outputs from embed_go_terms().\n",
        "      - model: The embedding model.\n",
        "      - threshold: Minimum cosine similarity required to accept the match.\n",
        "\n",
        "    Returns:\n",
        "      - best_go_id: The GO:ID of the best match (or None if score is below threshold)\n",
        "      - best_go_name: The GO term name of the best match.\n",
        "      - best_score: The cosine similarity score.\n",
        "    \"\"\"\n",
        "    # Encode the query using the model\n",
        "    query_embedding = model.encode(llama_term, convert_to_tensor=True)\n",
        "    # Compute cosine similarities between the query and all GO term embeddings\n",
        "    cos_scores = torch.squeeze(util.cos_sim(query_embedding, go_embeddings))\n",
        "    # Use torch.argmax to find the best matching index\n",
        "    best_idx = torch.argmax(cos_scores)\n",
        "    best_score = cos_scores[best_idx].item()\n",
        "\n",
        "    if best_score >= threshold:\n",
        "        return go_ids[best_idx], go_names[best_idx], best_score, best_idx\n",
        "    else:\n",
        "        return None, None, best_score, None\n",
        "\n",
        "# Test the matching function with an example Llama-generated term:\n",
        "example_llama_term = \"muscle cell differentiation\"\n",
        "best_go_id, best_go_name, score, best_idx = get_best_matching_go_term(example_llama_term, go_ids, go_names, go_embeddings, sapbert_model)\n",
        "print(f\"Example: Llama term '{example_llama_term}' matched with '{best_go_name}' ({best_go_id}) with score {score:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zp0nhP0_3pHz",
        "outputId": "8929c1c2-210f-48e0-d0fd-3d49c63adf2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'gene': 'glial fibrillary acidic protein', 'go_data': [{'term': 'intermediate filament', 'relationship': 'enables', 'namespace': 'cellular_component', 'justification': 'GFAP is expressed exclusively in astrocytes and is a type of intermediate filament protein.'}, {'term': 'astrocyte differentiation', 'relationship': 'involved_in', 'namespace': 'biological_process', 'justification': 'GFAP is considered to be important for the induction and maintenance of astrocyte differentiation.'}, {'term': 'regulation of cell proliferation', 'relationship': 'involved_in', 'namespace': 'biological_process', 'justification': 'Introduction of GFAP to C6 cells and human astrocytoma induced morphological changes, and suppressed their proliferation.'}, {'term': 'cytoskeleton organization', 'relationship': 'involved_in', 'namespace': 'biological_process', 'justification': 'GFAP is a type of intermediate filament protein that provides structural support to astrocytes.'}, {'term': 'central nervous system development', 'relationship': 'involved_in', 'namespace': 'biological_process', 'justification': 'GFAP is expressed during organogenesis in the central nervous system and is involved in the development of astrocytes.'}]}]\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "def parse_llama_output(llama_output_path):\n",
        "    \"\"\"\n",
        "    Parses Llama's output JSON file.\n",
        "    Returns the parsed JSON data (a list of gene entries).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(llama_output_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "        # If the JSON is a single dictionary, convert it to a list\n",
        "        if isinstance(data, dict):\n",
        "            data = [data]\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing JSON: {e}\")\n",
        "        return []\n",
        "\n",
        "print(parse_llama_output(\"/content/llama_output.json\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpyVuCmM_EtU",
        "outputId": "89fed3c5-49bf-4561-d69c-5135bcc70e5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gene: glial fibrillary acidic protein\n",
            "  Llama Term: intermediate filament\n",
            "  Matched GO Term: intermediate filament (GO:0005882)\n",
            "  Similarity: 1.00\n",
            "  Relationship: ['part_of GO:0045111']\n",
            "  Namespace: cellular_component\n",
            "  Justification: GFAP is expressed exclusively in astrocytes and is a type of intermediate filament protein.\n",
            "\n",
            "  Llama Term: astrocyte differentiation\n",
            "  Matched GO Term: astrocyte differentiation (GO:0048708)\n",
            "  Similarity: 1.00\n",
            "  Relationship: ['part_of GO:0007417']\n",
            "  Namespace: biological_process\n",
            "  Justification: GFAP is considered to be important for the induction and maintenance of astrocyte differentiation.\n",
            "\n",
            "  Llama Term: regulation of cell proliferation\n",
            "  Matched GO Term: regulation of cell population proliferation (GO:0042127)\n",
            "  Similarity: 0.93\n",
            "  Relationship: ['regulates GO:0008283']\n",
            "  Namespace: biological_process\n",
            "  Justification: Introduction of GFAP to C6 cells and human astrocytoma induced morphological changes, and suppressed their proliferation.\n",
            "\n",
            "  Llama Term: cytoskeleton organization\n",
            "  Matched GO Term: cytoskeleton organization (GO:0007010)\n",
            "  Similarity: 1.00\n",
            "  Relationship: unknown\n",
            "  Namespace: biological_process\n",
            "  Justification: GFAP is a type of intermediate filament protein that provides structural support to astrocytes.\n",
            "\n",
            "  Llama Term: central nervous system development\n",
            "  Matched GO Term: central nervous system development (GO:0007417)\n",
            "  Similarity: 1.00\n",
            "  Relationship: ['part_of GO:0007399']\n",
            "  Namespace: biological_process\n",
            "  Justification: GFAP is expressed during organogenesis in the central nervous system and is involved in the development of astrocytes.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def auto_match_go_terms(llama_data, go_ids, go_names, go_embeddings, go_namespaces, go_relationships, model, threshold=0.6):\n",
        "    \"\"\"\n",
        "    Given parsed Llama JSON data (with keys \"gene\" and \"go_data\" containing GO term details),\n",
        "    match each Llama-generated GO term (using its \"term\" field) to the best official GO term from GO.obo.\n",
        "\n",
        "    For each gene, returns a list of unique matches. Each match includes:\n",
        "      - The original Llama term.\n",
        "      - The matched official GO term (ID and name).\n",
        "      - The similarity score.\n",
        "      - The official relationship and namespace from GO.obo.\n",
        "      - The justification from the Llama output.\n",
        "\n",
        "    Only matches with a similarity above the threshold are kept.\n",
        "    \"\"\"\n",
        "    matched_go_terms = {}\n",
        "\n",
        "    for gene_entry in llama_data:\n",
        "        gene_name = gene_entry.get(\"gene\", \"UnknownGene\")\n",
        "        go_data = gene_entry.get(\"go_data\", [])\n",
        "        unique_matches = {}\n",
        "\n",
        "        for entry in go_data:\n",
        "            llama_term = entry.get(\"term\")\n",
        "            if not llama_term:\n",
        "                continue\n",
        "\n",
        "            # Use the Llama term to find the best matching GO term.\n",
        "            best_go_id, best_go_name, best_score, best_idx = get_best_matching_go_term(\n",
        "                llama_term, go_ids, go_names, go_embeddings, model, threshold=threshold\n",
        "            )\n",
        "\n",
        "            # Only add the match if it meets the threshold.\n",
        "            if best_go_id and best_idx is not None:\n",
        "                official_relationship = go_relationships[best_idx]\n",
        "                official_namespace = go_namespaces[best_idx]\n",
        "\n",
        "                # If this GO term was already matched, keep the one with the higher score.\n",
        "                if best_go_id in unique_matches:\n",
        "                    if best_score > unique_matches[best_go_id][\"similarity\"]:\n",
        "                        unique_matches[best_go_id] = {\n",
        "                            \"llama_term\": llama_term,\n",
        "                            \"matched_go_id\": best_go_id,\n",
        "                            \"matched_go_name\": best_go_name,\n",
        "                            \"similarity\": best_score,\n",
        "                            \"relationship\": official_relationship,\n",
        "                            \"namespace\": official_namespace,\n",
        "                            \"justification\": entry.get(\"justification\")\n",
        "                        }\n",
        "                else:\n",
        "                    unique_matches[best_go_id] = {\n",
        "                        \"llama_term\": llama_term,\n",
        "                        \"matched_go_id\": best_go_id,\n",
        "                        \"matched_go_name\": best_go_name,\n",
        "                        \"similarity\": best_score,\n",
        "                        \"relationship\": official_relationship,\n",
        "                        \"namespace\": official_namespace,\n",
        "                        \"justification\": entry.get(\"justification\")\n",
        "                    }\n",
        "\n",
        "        matched_go_terms[gene_name] = list(unique_matches.values())\n",
        "\n",
        "    return matched_go_terms\n",
        "\n",
        "# Usage example:\n",
        "llama_output_path = \"/content/llama_output.json\"\n",
        "llama_data = parse_llama_output(llama_output_path)\n",
        "\n",
        "matched_terms = auto_match_go_terms(\n",
        "    llama_data, go_ids, go_names, go_embeddings, go_namespaces, go_relationships, sapbert_model, threshold=0.6\n",
        ")\n",
        "\n",
        "# Output the results for each gene:\n",
        "for gene, matches in matched_terms.items():\n",
        "    print(f\"Gene: {gene}\")\n",
        "    for match in matches:\n",
        "        print(f\"  Llama Term: {match['llama_term']}\")\n",
        "        print(f\"  Matched GO Term: {match['matched_go_name']} ({match['matched_go_id']})\")\n",
        "        print(f\"  Similarity: {match['similarity']:.2f}\")\n",
        "        print(f\"  Relationship: {match['relationship']}\")\n",
        "        print(f\"  Namespace: {match['namespace']}\")\n",
        "        print(f\"  Justification: {match['justification']}\\n\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "6oaKSo6iXGOc"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "077e993516bb4e68ae51101f6b9c2530": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0a2df4b0204043feac1ab9887878b3c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0b3cdc367d354839ba4b3aa843be3ae5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10ff22b530bb4a609c9926814c0745f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1b30cf6534d5419380bb73aa2e4e6eae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1fec01a7236d4cf3acad0bd0c2dd402e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "200bcf0b84634d568d89319d3828a012": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "21ffdc6e9de147128fa39f9cd58d7995": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3bc3505975e64aaf9bf96e65770de232",
            "placeholder": "​",
            "style": "IPY_MODEL_b8cebd6679054a39abbaca928d63dbd0",
            "value": " 112/112 [00:00&lt;00:00, 2.72kB/s]"
          }
        },
        "239d1ef8640a40a8a043b171d7e3462f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25ac4acfb3b242358c24fcff69447b1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "34fd89eff8534c5ab156b9973e2374cc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3bad79220be54a87a02cdbac6c6c8bf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61616813f2f148e296978d5db567b3a7",
            "placeholder": "​",
            "style": "IPY_MODEL_4273a1a31d7a4caf86e16587eca40af5",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "3bc3505975e64aaf9bf96e65770de232": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c258adebdb04a2d98457fa7278a917a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3d3efcf524b74d08b559c2dad2ea52d2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4273a1a31d7a4caf86e16587eca40af5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4d3d0d4c1b9f4dac9b95ea254b6f4333": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc7be00627af4721a9e8686017e772f8",
            "placeholder": "​",
            "style": "IPY_MODEL_10ff22b530bb4a609c9926814c0745f7",
            "value": "model.safetensors: 100%"
          }
        },
        "50fe181f565643a7b31e8c4baf67366a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_34fd89eff8534c5ab156b9973e2374cc",
            "placeholder": "​",
            "style": "IPY_MODEL_0a2df4b0204043feac1ab9887878b3c8",
            "value": " 226k/226k [00:00&lt;00:00, 2.72MB/s]"
          }
        },
        "54da763ba0714288975cafc0cd43a1f7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "558d35b72a8f477ea9cd83e5ce7f2552": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e8a3f461e0b421e8095b0c5c777e360": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f0fb5433a01427b992d132b7b6debf3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "61616813f2f148e296978d5db567b3a7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "627ec98728254f5eb3cd7fa2756dc113": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54da763ba0714288975cafc0cd43a1f7",
            "max": 437955508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9a98603d5ce74808a3c4069a447c50b6",
            "value": 437955508
          }
        },
        "657a5a1cd09b4ba1a5ba5992b1ee128b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cda625c7fcad401488ff4b6c0c0a5777",
            "max": 462,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bd339b4ef33448a8a9837ca47b8fc8f5",
            "value": 462
          }
        },
        "6a10408f822e4bddbf31cca55a029bc4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70e32991c01148c98d4dc22a43d8c965": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a10408f822e4bddbf31cca55a029bc4",
            "placeholder": "​",
            "style": "IPY_MODEL_8a1f9a797f1a4ffe8bfef4e72bdfa280",
            "value": " 198/198 [00:00&lt;00:00, 5.39kB/s]"
          }
        },
        "7111a86c1d7c47daa9fd93001a902020": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7ef8ea0dcca64b19bfd01d6c977ce0e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4f0d71a138c4efc95593bcb7842be76",
            "placeholder": "​",
            "style": "IPY_MODEL_1b30cf6534d5419380bb73aa2e4e6eae",
            "value": " 438M/438M [00:05&lt;00:00, 103MB/s]"
          }
        },
        "7fd41e8255bb4907ad2775eb6cb5f7d5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a1f9a797f1a4ffe8bfef4e72bdfa280": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8ac0c98ff232418aa7d417aea5495fb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8cce6c61b8864b0fa055237abe08006e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f01cb77ea90c4b2bb4821c13e3b56ed6",
              "IPY_MODEL_8ed8bc76be6a477699c9ab641493f82b",
              "IPY_MODEL_21ffdc6e9de147128fa39f9cd58d7995"
            ],
            "layout": "IPY_MODEL_1fec01a7236d4cf3acad0bd0c2dd402e"
          }
        },
        "8d049df4c983441d82ca8bfe855abe67": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ed8bc76be6a477699c9ab641493f82b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7fd41e8255bb4907ad2775eb6cb5f7d5",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_077e993516bb4e68ae51101f6b9c2530",
            "value": 112
          }
        },
        "91b58c570960466491693c617aecb809": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9a98603d5ce74808a3c4069a447c50b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9d924d12256b4fd4802c28e035cc9967": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f65573653f834d199bd6a300ce9fa4d1",
            "max": 226150,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_25ac4acfb3b242358c24fcff69447b1d",
            "value": 226150
          }
        },
        "9e2b233314004ba791d7df6cd70c21a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4d3d0d4c1b9f4dac9b95ea254b6f4333",
              "IPY_MODEL_627ec98728254f5eb3cd7fa2756dc113",
              "IPY_MODEL_7ef8ea0dcca64b19bfd01d6c977ce0e8"
            ],
            "layout": "IPY_MODEL_200bcf0b84634d568d89319d3828a012"
          }
        },
        "b8cebd6679054a39abbaca928d63dbd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bc7be00627af4721a9e8686017e772f8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd339b4ef33448a8a9837ca47b8fc8f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c7f16613bb3941beb112cff932eafef7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cce594e541324d27909cf970d4db170c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_558d35b72a8f477ea9cd83e5ce7f2552",
            "placeholder": "​",
            "style": "IPY_MODEL_8ac0c98ff232418aa7d417aea5495fb0",
            "value": " 462/462 [00:00&lt;00:00, 35.5kB/s]"
          }
        },
        "cda625c7fcad401488ff4b6c0c0a5777": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d99fc6f064ba439e9c00af09a2593416": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_239d1ef8640a40a8a043b171d7e3462f",
            "placeholder": "​",
            "style": "IPY_MODEL_91b58c570960466491693c617aecb809",
            "value": "vocab.txt: 100%"
          }
        },
        "df1d5475258c4e3db84b8d2a16022961": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3bad79220be54a87a02cdbac6c6c8bf8",
              "IPY_MODEL_e3d3dac2784c4841b41ab7d0fc982155",
              "IPY_MODEL_70e32991c01148c98d4dc22a43d8c965"
            ],
            "layout": "IPY_MODEL_5e8a3f461e0b421e8095b0c5c777e360"
          }
        },
        "e0269ea1bc8441c5b8cf44485569ec08": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d99fc6f064ba439e9c00af09a2593416",
              "IPY_MODEL_9d924d12256b4fd4802c28e035cc9967",
              "IPY_MODEL_50fe181f565643a7b31e8c4baf67366a"
            ],
            "layout": "IPY_MODEL_c7f16613bb3941beb112cff932eafef7"
          }
        },
        "e3d3dac2784c4841b41ab7d0fc982155": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d049df4c983441d82ca8bfe855abe67",
            "max": 198,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5f0fb5433a01427b992d132b7b6debf3",
            "value": 198
          }
        },
        "e4f0d71a138c4efc95593bcb7842be76": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6c69cb9aa114f5c918981f26790bbf5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f01cb77ea90c4b2bb4821c13e3b56ed6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d3efcf524b74d08b559c2dad2ea52d2",
            "placeholder": "​",
            "style": "IPY_MODEL_3c258adebdb04a2d98457fa7278a917a",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "f65573653f834d199bd6a300ce9fa4d1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6baa2c31fb44fa5bfdeb94123fcf496": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6c69cb9aa114f5c918981f26790bbf5",
            "placeholder": "​",
            "style": "IPY_MODEL_7111a86c1d7c47daa9fd93001a902020",
            "value": "config.json: 100%"
          }
        },
        "f7d1ab90bc2f4b29a9a5e742a0e35e28": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f6baa2c31fb44fa5bfdeb94123fcf496",
              "IPY_MODEL_657a5a1cd09b4ba1a5ba5992b1ee128b",
              "IPY_MODEL_cce594e541324d27909cf970d4db170c"
            ],
            "layout": "IPY_MODEL_0b3cdc367d354839ba4b3aa843be3ae5"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
