{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2G7k3ZnFqp6J",
        "outputId": "c57f4f42-d167-4560-8aa3-dfb09f63cfee"
      },
      "outputs": [],
      "source": [
        "import gzip\n",
        "import os\n",
        "import requests\n",
        "import time\n",
        "\n",
        "def filter_matching_pmids(gene_gz, mutation_gz, output_file):\n",
        "    \"\"\"\n",
        "    Filters PMIDs that are present in both gene2pubtator3 and mutation2pubtator3 files.\n",
        "\n",
        "    Args:\n",
        "        gene_gz (str): Path to the gene2pubtator3.gz file.\n",
        "        mutation_gz (str): Path to the mutation2pubtator3.gz file.\n",
        "        output_file (str): Path to the output file for matched PMIDs.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Read gene2pubtator3 PMIDs\n",
        "        with gzip.open(gene_gz, 'rt', encoding='utf-8') as gene_file:\n",
        "            gene_pmids = set(line.split('\\t')[0] for line in gene_file)\n",
        "\n",
        "        # Read mutation2pubtator3 PMIDs and find intersection\n",
        "        matching_pmids = set()\n",
        "        with gzip.open(mutation_gz, 'rt', encoding='utf-8') as mutation_file:\n",
        "            for line in mutation_file:\n",
        "                pmid = line.split('\\t')[0]\n",
        "                if pmid in gene_pmids:\n",
        "                    matching_pmids.add(pmid)\n",
        "\n",
        "        # Write matching PMIDs to output file\n",
        "        with open(output_file, 'w', encoding='utf-8') as out_file:\n",
        "            for pmid in matching_pmids:\n",
        "                out_file.write(f\"{pmid}\\n\")\n",
        "\n",
        "        print(f\"Successfully wrote {len(matching_pmids)} matching PMIDs to {output_file}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error filtering PMIDs: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BlTiSM1imKU",
        "outputId": "95c03cfd-b6f3-449a-8962-a33079c83360"
      },
      "outputs": [],
      "source": [
        "import gzip\n",
        "import os\n",
        "import requests\n",
        "import time\n",
        "\n",
        "def fetch_articles(pmids_file, output_dir, email, api_key):\n",
        "    \"\"\"\n",
        "    Fetches article metadata and abstracts using the PubMed API for a list of PMIDs.\n",
        "\n",
        "    Args:\n",
        "        pmids_file (str): Path to the file containing PMIDs.\n",
        "        output_dir (str): Directory to save the fetched articles.\n",
        "        email (str): Email address for the PubMed API.\n",
        "        api_key (str): API key for the PubMed API.\n",
        "    \"\"\"\n",
        "    base_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\"\n",
        "    headers = {\"User-Agent\": email}\n",
        "    failure_log = \"failed_fetches.txt\"\n",
        "\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    try:\n",
        "        with open(pmids_file, 'r', encoding='utf-8') as file:\n",
        "            pmids = [line.strip() for line in file]\n",
        "\n",
        "        for i, pmid in enumerate(pmids):\n",
        "            final_output_path = os.path.join(output_dir, f\"{pmid}.txt\")\n",
        "            temp_output_path = os.path.join(output_dir, f\"{pmid}.tmp\")\n",
        "            if os.path.exists(final_output_path):\n",
        "                print(f\"PMID {pmid} already fetched. Skipping ({i+1}/{len(pmids)})\")\n",
        "                continue\n",
        "\n",
        "            params = {\n",
        "                \"db\": \"pubmed\",\n",
        "                \"id\": pmid,\n",
        "                \"rettype\": \"abstract\",\n",
        "                \"retmode\": \"text\",\n",
        "                \"api_key\": api_key\n",
        "            }\n",
        "            try:\n",
        "                response = requests.get(base_url, headers=headers, params=params)\n",
        "\n",
        "                if response.status_code == 200:\n",
        "                    with open(temp_output_path, 'w', encoding='utf-8') as temp_file:\n",
        "                        temp_file.write(response.text)\n",
        "                    os.rename(temp_output_path, final_output_path)\n",
        "                    print(f\"Fetched article for PMID {pmid} ({i+1}/{len(pmids)})\")\n",
        "                else:\n",
        "                    print(f\"Failed to fetch PMID {pmid}: {response.status_code}\")\n",
        "                    with open(failure_log, 'a', encoding='utf-8') as fail_file:\n",
        "                        fail_file.write(f\"{pmid}\\n\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error fetching PMID {pmid}: {e}\")\n",
        "                with open(failure_log, 'a', encoding='utf-8') as fail_file:\n",
        "                    fail_file.write(f\"{pmid}\\n\")\n",
        "\n",
        "            time.sleep(0.13)  # Reduced sleep time to increase request rate\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching articles: {e}\")\n",
        "\n",
        "def main():\n",
        "    # Paths to input files\n",
        "    gene_gz = \"./data/gene2pubtator3.gz\"\n",
        "    mutation_gz = \"./data/mutation2pubtator3.gz\"\n",
        "\n",
        "    # Path to output file for matching PMIDs\n",
        "    matching_pmids_file = \"./data/matching_pmids.txt\"\n",
        "\n",
        "    # Directory to save fetched articles\n",
        "    articles_dir = \"./data/fetched_articles\"\n",
        "\n",
        "    # PubMed API credentials\n",
        "    email = \"\"  # Replace with your email\n",
        "    api_key = \"\"  # Replace with your API key\n",
        "\n",
        "    # Filter and write matching PMIDs\n",
        "    #filter_matching_pmids(gene_gz, mutation_gz, matching_pmids_file)\n",
        "\n",
        "    # Fetch articles for matching PMIDs\n",
        "    fetch_articles(matching_pmids_file, articles_dir, email, api_key)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7jEpzqdiJyI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetched single PMID 35351360 in XML format.\n"
          ]
        }
      ],
      "source": [
        "import gzip\n",
        "import os\n",
        "import requests\n",
        "import time\n",
        "\n",
        "def fetch_articles(pmids_file, output_dir, email, api_key, failure_log_dir):\n",
        "    \"\"\"\n",
        "    Fetches article metadata and abstracts using the PubMed API for a list of PMIDs.\n",
        "\n",
        "    Args:\n",
        "        pmids_file (str): Path to the file containing PMIDs.\n",
        "        output_dir (str): Directory to save the fetched articles.\n",
        "        email (str): Email address for the PubMed API.\n",
        "        api_key (str): API key for the PubMed API.\n",
        "        failure_log_dir (str): Directory to save the failure logs.\n",
        "    \"\"\"\n",
        "    base_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\"\n",
        "    headers = {\"User-Agent\": email}\n",
        "    subset_name = os.path.splitext(os.path.basename(pmids_file))[0]\n",
        "    failure_log = os.path.join(failure_log_dir, f\"{subset_name}_failed_fetches.txt\")\n",
        "\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    if not os.path.exists(failure_log_dir):\n",
        "        os.makedirs(failure_log_dir)\n",
        "\n",
        "    try:\n",
        "        with open(pmids_file, 'r', encoding='utf-8') as file:\n",
        "            pmids = [line.strip() for line in file]\n",
        "\n",
        "        for i, pmid in enumerate(pmids):\n",
        "            final_output_path = os.path.join(output_dir, f\"{pmid}.txt\")\n",
        "            temp_output_path = os.path.join(output_dir, f\"{pmid}.tmp\")\n",
        "            if os.path.exists(final_output_path):\n",
        "                print(f\"PMID {pmid} already fetched. Skipping ({i+1}/{len(pmids)})\")\n",
        "                continue\n",
        "\n",
        "            # https://www.ncbi.nlm.nih.gov/books/NBK25499/table/chapter4.T._valid_values_of__retmode_and/?report=objectonly\n",
        "            params = {\n",
        "                \"db\": \"pubmed\",\n",
        "                \"id\": pmid,\n",
        "                \"rettype\": \"abstract\",\n",
        "                \"retmode\": \"text\",\n",
        "                \"api_key\": api_key\n",
        "            }\n",
        "            try:\n",
        "                response = requests.get(base_url, headers=headers, params=params)\n",
        "\n",
        "                if response.status_code == 200:\n",
        "                    with open(temp_output_path, 'w', encoding='utf-8') as temp_file:\n",
        "                        temp_file.write(response.text)\n",
        "                    os.rename(temp_output_path, final_output_path)\n",
        "                    # print(f\"Fetched article for PMID {pmid} ({i+1}/{len(pmids)})\")\n",
        "                else:\n",
        "                    print(f\"Failed to fetch PMID {pmid}: {response.status_code}\")\n",
        "                    with open(failure_log, 'a', encoding='utf-8') as fail_file:\n",
        "                        fail_file.write(f\"{pmid}\\n\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error fetching PMID {pmid}: {e}\")\n",
        "                with open(failure_log, 'a', encoding='utf-8') as fail_file:\n",
        "                    fail_file.write(f\"{pmid}\\n\")\n",
        "\n",
        "            time.sleep(0.1105)  # Reduced sleep time to increase request rate\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching articles: {e}\")\n",
        "\n",
        "def main():\n",
        "    # # Directory containing subset files\n",
        "    # subsets_dir = \"./data/matching_pmids_subsets\"\n",
        "    \n",
        "    # # Directory to save fetched articles\n",
        "    # fetched_articles_subsets_dir = \"./data/fetched_articles_subsets\"\n",
        "    \n",
        "    # Directory to save failure logs\n",
        "    failure_log_dir = \"./data/failed_fetches\"\n",
        "    \n",
        "    # PubMed API credentials\n",
        "    email = \"\"  # Replace with your email\n",
        "    api_key = \"\"  # Replace with your API key\n",
        "\n",
        "    # Iterate over each subset file and fetch articles\n",
        "    for subset_file in os.listdir(subsets_dir):\n",
        "        print(f\"Processing subset file: {subset_file}\")\n",
        "        start_time = time.time()\n",
        "        if subset_file.endswith(\".txt\"):\n",
        "            subset_path = os.path.join(subsets_dir, subset_file)\n",
        "            subset_output_dir = os.path.join(fetched_articles_subsets_dir, os.path.splitext(subset_file)[0])\n",
        "            fetch_articles(subset_path, subset_output_dir, email, api_key, failure_log_dir)\n",
        "        print(f\"Finished processing subset file: {subset_file}. Took {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fetch full-text articles in BioC format from PMC OA. \n",
        "The reason why I didn't use entrez is because the full-text articles fetched from it is in JATS XML which Pubtator3/AIONER can't process.I used BioC API for PMC Open Access instead [link here](https://www.ncbi.nlm.nih.gov/research/bionlp/APIs/BioC-PMC/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully fetched article 35351360.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import requests\n",
        "import time\n",
        "from tqdm import tqdm  # Progress bar\n",
        "\n",
        "def fetch_pmc_articles(pmcids_file, output_dir, failure_log_dir):\n",
        "    \"\"\"\n",
        "    Fetches full-text PMC articles in BioC XML format using the BioC API.\n",
        "\n",
        "    Args:\n",
        "        pmcids_file (str): Path to the file containing PMCIDs.\n",
        "        output_dir (str): Directory to save the fetched articles.\n",
        "        failure_log_dir (str): Directory to save the failure logs.\n",
        "    \"\"\"\n",
        "    # ascii instead of unicode for easier processing and compatibility\n",
        "    base_url = \"https://www.ncbi.nlm.nih.gov/research/bionlp/RESTful/pmcoa.cgi/BioC_xml/{}/ascii\"\n",
        "    \n",
        "    subset_name = os.path.splitext(os.path.basename(pmcids_file))[0]\n",
        "    failure_log = os.path.join(failure_log_dir, f\"{subset_name}_failed_fetches.txt\")\n",
        "\n",
        "    # Ensure directories exist\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    os.makedirs(failure_log_dir, exist_ok=True)\n",
        "\n",
        "    try:\n",
        "        with open(pmcids_file, 'r', encoding='utf-8') as file:\n",
        "            pmcids = [line.strip().split(\"\\t\")[1] for line in file if line.strip()]  # Extract only PMCIDs\n",
        "\n",
        "        # Initialize tqdm progress bar\n",
        "        for i, pmcid in enumerate(tqdm(pmcids, desc=f\"Downloading {subset_name}\", unit=\"article\")):\n",
        "            final_output_path = os.path.join(output_dir, f\"{pmcid}.xml\")\n",
        "            temp_output_path = os.path.join(output_dir, f\"{pmcid}.tmp\")\n",
        "\n",
        "            # Skip if already fetched\n",
        "            if os.path.exists(final_output_path):\n",
        "                continue\n",
        "\n",
        "            # Fetch article from BioC API\n",
        "            url = base_url.format(pmcid)\n",
        "            try:\n",
        "                response = requests.get(url, timeout=10)\n",
        "\n",
        "                if response.status_code == 200:\n",
        "                    with open(temp_output_path, 'w', encoding='utf-8') as temp_file:\n",
        "                        temp_file.write(response.text)\n",
        "                    os.rename(temp_output_path, final_output_path)  # Rename after successful fetch\n",
        "                else:\n",
        "                    with open(failure_log, 'a', encoding='utf-8') as fail_file:\n",
        "                        fail_file.write(f\"{pmcid}\\n\")\n",
        "\n",
        "            except Exception as e:\n",
        "                with open(failure_log, 'a', encoding='utf-8') as fail_file:\n",
        "                    fail_file.write(f\"{pmcid}\\n\")\n",
        "\n",
        "            time.sleep(0.5)  # Prevent overloading the API\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error processing {pmcids_file}: {e}\")\n",
        "\n",
        "def fetch_single_pmc_article(pmcid, output_dir, failure_log_dir):\n",
        "    \"\"\"\n",
        "    Fetches a single full-text PMC article in BioC XML format using the BioC API.\n",
        "\n",
        "    Args:\n",
        "        pmcid (str): The PMCID of the article to fetch.\n",
        "        output_dir (str): Directory to save the fetched article.\n",
        "        failure_log_dir (str): Directory to save the failure log.\n",
        "    \"\"\"\n",
        "    base_url = \"https://www.ncbi.nlm.nih.gov/research/bionlp/RESTful/pmcoa.cgi/BioC_xml/{}/ascii\"\n",
        "    failure_log = os.path.join(failure_log_dir, \"failed_fetches.txt\")\n",
        "\n",
        "    # Ensure directories exist\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    os.makedirs(failure_log_dir, exist_ok=True)\n",
        "\n",
        "    final_output_path = os.path.join(output_dir, f\"{pmcid}.xml\")\n",
        "    temp_output_path = os.path.join(output_dir, f\"{pmcid}.tmp\")\n",
        "\n",
        "    # Skip if already fetched\n",
        "    if os.path.exists(final_output_path):\n",
        "        print(f\"Article {pmcid} already fetched.\")\n",
        "        return\n",
        "\n",
        "    # Fetch article from BioC API\n",
        "    url = base_url.format(pmcid)\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            with open(temp_output_path, 'w', encoding='utf-8') as temp_file:\n",
        "                temp_file.write(response.text)\n",
        "            os.rename(temp_output_path, final_output_path)  # Rename after successful fetch\n",
        "            print(f\"Successfully fetched article {pmcid}.\")\n",
        "        else:\n",
        "            with open(failure_log, 'a', encoding='utf-8') as fail_file:\n",
        "                fail_file.write(f\"{pmcid}\\n\")\n",
        "            print(f\"Failed to fetch article {pmcid}.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        with open(failure_log, 'a', encoding='utf-8') as fail_file:\n",
        "            fail_file.write(f\"{pmcid}\\n\")\n",
        "        print(f\"Error fetching article {pmcid}: {e}\")\n",
        "\n",
        "    time.sleep(0.5)  # Prevent overloading the API\n",
        "\n",
        "def fetch_single_pmid_article(pmid, output_dir, failure_log_dir):\n",
        "    \"\"\"\n",
        "    Fetches a single article in BioC XML format for a given PMID using the RESTful service.\n",
        "    \"\"\"\n",
        "    base_url = \"https://www.ncbi.nlm.nih.gov/research/bionlp/RESTful/pubmed.cgi/BioC_xml/{}/ascii\"\n",
        "    failure_log = os.path.join(failure_log_dir, \"failed_fetches.txt\")\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    os.makedirs(failure_log_dir, exist_ok=True)\n",
        "\n",
        "    final_output_path = os.path.join(output_dir, f\"{pmid}.xml\")\n",
        "    temp_output_path = os.path.join(output_dir, f\"{pmid}.tmp\")\n",
        "\n",
        "    if os.path.exists(final_output_path):\n",
        "        print(f\"Article {pmid} already fetched.\")\n",
        "        return\n",
        "\n",
        "    url = base_url.format(pmid)\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "        if response.status_code == 200 and response.text.startswith('<?xml'):\n",
        "            with open(temp_output_path, 'w', encoding='utf-8') as temp_file:\n",
        "                temp_file.write(response.text)\n",
        "            os.rename(temp_output_path, final_output_path)\n",
        "            print(f\"Successfully fetched article {pmid}.\")\n",
        "        else:\n",
        "            with open(failure_log, 'a', encoding='utf-8') as fail_file:\n",
        "                fail_file.write(f\"{pmid}\\n\")\n",
        "            print(f\"Failed to fetch article {pmid}.\")\n",
        "    except Exception as e:\n",
        "        with open(failure_log, 'a', encoding='utf-8') as fail_file:\n",
        "            fail_file.write(f\"{pmid}\\n\")\n",
        "        print(f\"Error fetching article {pmid}: {e}\")\n",
        "\n",
        "    time.sleep(0.5)\n",
        "\n",
        "def main():\n",
        "    # # ============================================ Fetch full-text PMC articles (set of full texts) ============================================\n",
        "    # # Directory containing subset files\n",
        "    # subsets_dir = r\"C:\\Users\\aivan\\Desktop\\BIOIN 401\\GOLLM\\data\\matching_pmcids_subsets\"\n",
        "\n",
        "    # # Directory to save fetched full-text articles\n",
        "    # fetched_articles_subsets_dir = r\"C:\\Users\\aivan\\Desktop\\BIOIN 401\\GOLLM\\data\\fetched_full_articles_subsets\"\n",
        "\n",
        "    # # Directory to save failure logs\n",
        "    # failure_log_dir = r\"C:\\Users\\aivan\\Desktop\\BIOIN 401\\GOLLM\\data\\failed_full_text_fetches\"\n",
        "\n",
        "    # # Iterate over each subset file and fetch articles\n",
        "    # for subset_file in os.listdir(subsets_dir):\n",
        "    #     print(f\"\\n📂 Processing subset file: {subset_file}\")\n",
        "    #     start_time = time.time()\n",
        "\n",
        "    #     if subset_file.endswith(\".txt\"):\n",
        "    #         subset_path = os.path.join(subsets_dir, subset_file)\n",
        "    #         subset_output_dir = os.path.join(fetched_articles_subsets_dir, os.path.splitext(subset_file)[0])\n",
        "\n",
        "    #         fetch_pmc_articles(subset_path, subset_output_dir, failure_log_dir)\n",
        "\n",
        "    #     print(f\"✅ Finished processing {subset_file}. Took {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "    # # ============================================ Fetch a single PMC article (full text) ============================================\n",
        "    # pmcid = \"PMC1866366\"\n",
        "    # output_dir = r\"C:\\Users\\aivan\\Desktop\\BIOIN 401\\GOLLM\\data\\fetched_single_article\"\n",
        "    # failure_log_dir = r\"C:\\Users\\aivan\\Desktop\\BIOIN 401\\GOLLM\\data\\failed_single_article_fetch\"\n",
        "\n",
        "    # fetch_single_pmc_article(pmcid, output_dir, failure_log_dir)\n",
        "\n",
        "    # ============================================ Fetch a single PMID article (abstract) ============================================\n",
        "    pmid = \"35351360\"\n",
        "    output_dir = r\"C:\\Users\\aivan\\Desktop\\BIOIN 401\\GOLLM\\data\\fetched_single_article\"\n",
        "    failure_log_dir = r\"C:\\Users\\aivan\\Desktop\\BIOIN 401\\GOLLM\\data\\failed_single_article_fetch\"\n",
        "    fetch_single_pmid_article(pmid, output_dir, failure_log_dir)\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "BIOIN311C1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
