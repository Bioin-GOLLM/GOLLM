1. Bioinformatics. 2023 Mar 1;39(3):btad103. doi: 10.1093/bioinformatics/btad103.

On the effectiveness of compact biomedical transformers.

Rohanian O(1)(2), Nouriborji M(2), Kouchaki S(3), Clifton DA(1)(4).

Author information:
(1)Department of Engineering Science, University of Oxford, Oxford, UK.
(2)NLPie Research, Oxford, UK.
(3)Department of Electrical and Electronic Engineering, University of Surrey, 
Guildford, UK.
(4)Oxford-Suzhou Centre for Advanced Research, Suzhou, China.

MOTIVATION: Language models pre-trained on biomedical corpora, such as BioBERT, 
have recently shown promising results on downstream biomedical tasks. Many 
existing pre-trained models, on the other hand, are resource-intensive and 
computationally heavy owing to factors such as embedding size, hidden dimension 
and number of layers. The natural language processing community has developed 
numerous strategies to compress these models utilizing techniques such as 
pruning, quantization and knowledge distillation, resulting in models that are 
considerably faster, smaller and subsequently easier to use in practice. By the 
same token, in this article, we introduce six lightweight models, namely, 
BioDistilBERT, BioTinyBERT, BioMobileBERT, DistilBioBERT, TinyBioBERT and 
CompactBioBERT which are obtained either by knowledge distillation from a 
biomedical teacher or continual learning on the Pubmed dataset. We evaluate all 
of our models on three biomedical tasks and compare them with BioBERT-v1.1 to 
create the best efficient lightweight models that perform on par with their 
larger counterparts.
RESULTS: We trained six different models in total, with the largest model having 
65 million in parameters and the smallest having 15 million; a far lower range 
of parameters compared with BioBERT's 110M. Based on our experiments on three 
different biomedical tasks, we found that models distilled from a biomedical 
teacher and models that have been additionally pre-trained on the PubMed dataset 
can retain up to 98.8% and 98.6% of the performance of the BioBERT-v1.1, 
respectively. Overall, our best model below 30 M parameters is BioMobileBERT, 
while our best models over 30 M parameters are DistilBioBERT and CompactBioBERT, 
which can keep up to 98.2% and 98.8% of the performance of the BioBERT-v1.1, 
respectively.
AVAILABILITY AND IMPLEMENTATION: Codes are available at: 
https://github.com/nlpie-research/Compact-Biomedical-Transformers. Trained 
models can be accessed at: https://huggingface.co/nlpie.

Â© The Author(s) 2023. Published by Oxford University Press.

DOI: 10.1093/bioinformatics/btad103
PMCID: PMC10027428
PMID: 36825820 [Indexed for MEDLINE]